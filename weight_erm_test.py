#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time    : 11/07/2017 15:04# @Author  : LucienWang# @File    : weight_erm_test.py__author__ = 'lucien'import csvimport os.path as pathimport sysimport multiprocessingfrom functools import partialfrom dataset_utils.graph_generator import *from utils.static import *from utils.tools import current_microsecondfrom weighting_utils.weighting import *from algorithms.logistic import *from algorithms.sklearn_models import *N_TRAIN_NODE =int(sys.argv[1])N_TEST_NODE = 300N_VISIBLE = 8N = 10BARABASI_M = 2cores = 15repeat_times = 50file_name = './worst.csv'GraphDataSet = GraphLinearDataSetModel = logistic_regression# Model = least_squares_ridge# generate_train_graph = generate_barabasigenerate_train_graph = generate_worst_graphgenerate_test_graph = generate_complete_graphweighting_scheme = worst_weighting# weighting_scheme = fmndef single_experiment(time, train_g, test_g, origin_weights, GraphDataSet,                      Model):    print(time)    random = np.random.RandomState(current_microsecond() + time)    train_graph = []    test_graph = []    w = random.uniform(-1, 1, size=N)    b = 0  # random.normal(-1, 1)    train_graph = GraphDataSet(train_g, N, random=random, w=w,                                     b=b)    test_graph = GraphDataSet(test_g, N, random=random, w=w,                                    b=b)    X_train, Y_train = train_graph.dataset(n_visible=N_VISIBLE)    X_test, Y_test = test_graph.dataset(n_visible=N_VISIBLE)    matching_weights = origin_weights    equal_acc, eqaul_train_acc = Model(X_train, Y_train, X_test, Y_test)    fmn_acc, fmn_train_acc = Model(X_train, Y_train, X_test, Y_test, matching_weights)    return (equal_acc, fmn_acc, eqaul_train_acc, fmn_train_acc)def single_stability_experiment(time, train_g, test_g, origin_weights, GraphDataSet,                      Model):    print(time)    random = np.random.RandomState(current_microsecond() + time)    train_graph = []    test_graph = []    w = random.uniform(-1, 1, size=(N, N))    b = 0  # random.normal(-1, 1)    train_graph = GraphDataSet(train_g, N, random=random, w=w,                                     b=b)    test_graph = GraphDataSet(test_g, N, random=random, w=w,                                    b=b)    X_train, Y_train = train_graph.dataset(n_visible=N_VISIBLE)    X_test, Y_test = test_graph.dataset(n_visible=N_VISIBLE)    matching_weights = origin_weights    equal_acc, eqaul_train_acc = Model(X_train, Y_train, X_test, Y_test)    fmn_acc, fmn_train_acc = Model(X_train, Y_train, X_test, Y_test, matching_weights)    return (equal_acc, fmn_acc, eqaul_train_acc, fmn_train_acc)def main():    pool = multiprocessing.Pool(cores)    list_times = range(repeat_times)    train_g = generate_train_graph(N_TRAIN_NODE, BARABASI_M)    print(len(train_g.nodes), len(train_g.edges))    test_g = generate_test_graph(N_TEST_NODE)    weights = weighting_scheme(train_g.nodes, train_g.edges)    results = pool.map(        partial(single_experiment, train_g=train_g, test_g=test_g, origin_weights=weights,                GraphDataSet=GraphDataSet,                Model=Model),        list_times)    pool.close()    pool.join()    results = np.asarray(results, dtype='float')    errors = np.mean(results, axis=0)    if not path.isfile(file_name):        with open(file_name, 'w') as f:            w = csv.writer(f)            w.writerow(['测试误差', '训练误差', '类型', 'b_m', 'd', 'n'])    with open(file_name, 'a') as f:        w = csv.writer(f)        for r in results:            w.writerow([r[0], r[2], '平均加权', BARABASI_M, N_VISIBLE, N_TRAIN_NODE])            w.writerow([r[1], r[3], '分数匹配加权', BARABASI_M, N_VISIBLE, N_TRAIN_NODE])    # print(np.std(results, axis=0))    print(errors)    print(errors[2] - errors[0], errors[3] - errors[1])if __name__ == '__main__':    # main_stability()    main()    # print(single_experiment(0, DatasetType.linear, ModelType.logistic_regression))